# -*- coding: utf-8 -*-
"""HumanActivityRecognitionConvLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGlf-3Rq8WyTSXhX5BVwcsglaCR62Jfb
"""

!pip install pafy youtube-dl moviepy
!pip install imageio-ffmpeg

import os 
import cv2
import pafy
import math
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt
import imageio_ffmpeg
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model

import random
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)

!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar

!unrar x UCF50.rar

#Create Matplotlib figure and specify the size of the figure
plt.figure(figsize = (20,20))

#Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('UCF50')

random_range = random.sample(range(len(all_classes_names)), 20) #Generating 20 sample class for visualization of images

#iterating through all the generated random values.
for counter,random_index in enumerate(random_range,1):

  #Retrieve a class name using the Random Index.
  selected_class_name = all_classes_names[random_index]
  
  #Retrieve the list of all the video files present in the randomly selected Class Directory
  video_files_names_list = os.listdir(f'UCF50/{selected_class_name}')

  #Randomly select a video file from the list retrieved from the randomly selected class directory.
  selected_video_file_name = random.choice(video_files_names_list)
  
  #Initialize a VideoCapture object to read from the video file.
  video_reader = cv2.VideoCapture(f'UCF50/{selected_class_name}/{selected_video_file_name}')

  #Read the first frame of the video file.
  _, bgr_frame = video_reader.read()

  #Release the video capture object.
  video_reader.release()

  #Convert the frame from BGR to RGB format.
  rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

  #Write the class name on the video frame.
  cv2.putText(rgb_frame, selected_class_name, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)
  
  #Display the frame.
  plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')

"""##**Preprocess the dataset**

"""

IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64

#Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 64

DATASET_DIR = 'UCF50'

#Specify the list containing the names of the classes used for training. 
Classes_lıst = ['WalkingWithDog', 'TaiChi', 'Swing', "HorseRace"]

"""##**Create a function to Extract, Resize & Normalize Frames**"""

def extract_and_preprocess_frames(video_path):
  #List to store frames
  frames_list = []

  #Read the video file using the VideoCapture object.
  video_reader = cv2.VideoCapture(video_path)

  #Get the total number of frames in the video.
  total_frame_number = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

  #Calculate the interval after which frames will be added to the list.
  frame_jump = max((total_frame_number // SEQUENCE_LENGTH),1)

  # Iterate through the Video Frames.

  for i in range(SEQUENCE_LENGTH):
    # Set the video reader to the total_frames/SequenceLength * i th frame and extract it
    video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_jump * i)

    #Reading the frame from the video.
    success, frame = video_reader.read()

    #Check if the video is not successfully read.
    if not success:
      break

    #Resize the frame to fixed height and width.
    resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

    # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
    normalized_frame = resized_frame / 255.0

    #Append the normalized frame into the frames list.
    frames_list.append(normalized_frame)

  video_reader.release()

  return frames_list

"""## **Create a Function for Dataset Creation**"""

def create_dataset():
  features = []
  labels = []
  video_file_paths = []

  # Iterating through selected classes for classification
  for class_index, class_name in enumerate(Classes_lıst):
    print(f'Extracting frames of a class {class_name}...')

    video_list = os.listdir(DATASET_DIR+'/'+class_name)

    # Iterate through all the videos 
    for file_name in video_list:

      video_file_path = os.path.join(DATASET_DIR,class_name,file_name)

      frames = extract_and_preprocess_frames(video_file_path)

      if len(frames) == SEQUENCE_LENGTH:
        features.append(frames)
        labels.append(class_index)
        video_file_paths.append(video_file_path)
  
  features = np.asarray(features)
  labels = np.array(labels)
  print(f'Shape of Input: {features.shape}') # (Batch Size, Sequence Length, {feature size = {Heıght, Wıdth, Channel}})
  print(f'shape of labels: {labels.shape}')
  
  return features, labels, video_file_paths

# Create dataset
input_data, labels, video_file_paths = create_dataset()

# One  Hot Encoded Labels
one_hot_encoded_labels = to_categorical(labels)

"""##**Split the data into train and test sets.**"""

train_x , test_x, train_y, test_y = train_test_split(input_data, 
                                                     one_hot_encoded_labels,
                                                     test_size=0.15,
                                                     shuffle=True, 
                                                     random_state=seed_constant)

"""##**Building ConvLSTM Model.**"""

def create_ConvLSTM():

  model = Sequential()
  
  model.add(ConvLSTM2D(filters=16, kernel_size=(3,3), activation='tanh', data_format='channels_last',
                       recurrent_dropout= 0.2, return_sequences = True, input_shape = (input_data.shape[1:])))
  
  model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))

  model.add(TimeDistributed(Dropout(0.2)))

  model.add(ConvLSTM2D(filters=32, kernel_size=(3,3), activation='tanh', data_format='channels_last',
                       recurrent_dropout=0.2, return_sequences=True))
  
  model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
  model.add(TimeDistributed(Dropout(0.2)))
  
  model.add(ConvLSTM2D(filters=32, kernel_size=(3,3), activation='tanh', data_format='channels_last',
                       recurrent_dropout=0.2, return_sequences=True))
  
  model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
  model.add(TimeDistributed(Dropout(0.2)))

  model.add(ConvLSTM2D(filters=32, kernel_size=(3,3), activation='tanh', data_format='channels_last',
                       recurrent_dropout=0.2, return_sequences=True))
  
  model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
  #model.add(TimeDistributed(Dropout(0.2)))
  model.add(Flatten())
  model.add(Dense(len(Classes_lıst), activation='softmax'))

  model.summary()
  return model

model = create_ConvLSTM()

"""##**Model Architecture**"""

plot_model(model, to_file='ConvLSTM.png', show_shapes=True, show_layer_names=True)

model.compile(loss="categorical_crossentropy", optimizer="Adam", metrics=["accuracy"])

history = model.fit(x = train_x, y = train_y, epochs = 50, batch_size = 16, shuffle=True, validation_split=0.1)

